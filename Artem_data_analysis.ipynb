{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf919d26-ece5-4672-ab0f-09ee7864600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in d:\\anaconda\\lib\\site-packages (14.0.2)\n",
      "Requirement already satisfied: numpy>=1.16.6 in d:\\anaconda\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Requirement already satisfied: xgboost in d:\\anaconda\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Файл train.parquet успешно загружен.\n"
     ]
    }
   ],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install pyarrow\n",
    "!pip install xgboost\n",
    "\n",
    "# Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# Функция для загрузки файлов с GitHub\n",
    "def download_file(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Файл {save_path} успешно загружен.\")\n",
    "    else:\n",
    "        print(f\"Ошибка при загрузке файла {save_path}: {response.status_code}\")\n",
    "\n",
    "# Загрузка файлов с GitHub\n",
    "url_train = \"https://github.com/Mechanic17/time_series_classification/raw/refs/heads/main/train.parquet\"\n",
    "url_test = \"https://github.com/Mechanic17/time_series_classification/raw/refs/heads/main/test.parquet\"\n",
    "url_sample_submission = \"https://raw.githubusercontent.com/Mechanic17/time_series_classification/refs/heads/main/sample_submission.csv\"\n",
    "\n",
    "download_file(url_train, 'train.parquet')\n",
    "download_file(url_test, 'test.parquet')\n",
    "download_file(url_sample_submission, 'sample_submission.csv')\n",
    "\n",
    "# Проверяем наличие файлов и читаем их\n",
    "try:\n",
    "    train_data = pd.read_parquet('train.parquet')\n",
    "    print(\"Данные train.parquet успешно загружены.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Файл train.parquet не найден.\")\n",
    "\n",
    "try:\n",
    "    test_data = pd.read_parquet('test.parquet')\n",
    "    print(\"Данные test.parquet успешно загружены.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Файл test.parquet не найден.\")\n",
    "\n",
    "try:\n",
    "    sample_submission = pd.read_csv('sample_submission.csv')\n",
    "    print(\"Данные sample_submission.csv успешно загружены.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Файл sample_submission.csv не найден.\")\n",
    "\n",
    "# Проверяем столбцы в каждом наборе данных\n",
    "print(\"\\nСтолбцы в train_data:\", train_data.columns.tolist())\n",
    "print(\"Столбцы в test_data:\", test_data.columns.tolist())\n",
    "print(\"Столбцы в sample_submission:\", sample_submission.columns.tolist())\n",
    "\n",
    "# Шаг 2: EDA - Анализ данных\n",
    "print(\"\\nПервые строки train_data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nИнформация о train_data:\")\n",
    "print(train_data.info())\n",
    "\n",
    "print(\"\\nОписание числовых признаков train_data:\")\n",
    "print(train_data.describe())\n",
    "\n",
    "# Распределение целевой переменной\n",
    "print(\"\\nРаспределение целевой переменной 'label':\")\n",
    "print(train_data['label'].value_counts())\n",
    "\n",
    "# Проверка корреляции признаков\n",
    "numerical_data = train_data.select_dtypes(include=[np.number])\n",
    "sns.heatmap(numerical_data.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Корреляция признаков\")\n",
    "plt.show()\n",
    "\n",
    "# Шаг 3: Предобработка данных\n",
    "\n",
    "# Сохраняем 'id' из test_data для формирования файла submission.csv\n",
    "test_ids = test_data['id']\n",
    "\n",
    "# Удаляем столбец 'id' из train_data и test_data\n",
    "train_data = train_data.drop('id', axis=1)\n",
    "test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "# Обработка столбца 'dates'\n",
    "def process_dates(df):\n",
    "    df['dates'] = df['dates'].apply(convert_to_list)\n",
    "    df['dates'] = df['dates'].apply(lambda x: [pd.to_datetime(d, errors='coerce') for d in x])\n",
    "    # Извлекаем признаки из дат\n",
    "    df['date_counts'] = df['dates'].apply(len)\n",
    "    df['date_min'] = df['dates'].apply(lambda x: min(x) if len(x) > 0 else pd.NaT)\n",
    "    df['date_max'] = df['dates'].apply(lambda x: max(x) if len(x) > 0 else pd.NaT)\n",
    "    df['date_range'] = (df['date_max'] - df['date_min']).dt.days.fillna(0)\n",
    "    # Удаляем временные столбцы\n",
    "    df.drop(['dates', 'date_min', 'date_max'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def convert_to_list(val):\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    elif isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except:\n",
    "            return []\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "train_data = process_dates(train_data)\n",
    "test_data = process_dates(test_data)\n",
    "\n",
    "# Обработка столбца 'values'\n",
    "def process_values(df):\n",
    "    df['values'] = df['values'].apply(convert_to_list)\n",
    "    # Извлекаем статистические признаки из 'values'\n",
    "    df['values_mean'] = df['values'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "    df['values_std'] = df['values'].apply(lambda x: np.std(x) if len(x) > 0 else 0)\n",
    "    df['values_min'] = df['values'].apply(lambda x: np.min(x) if len(x) > 0 else 0)\n",
    "    df['values_max'] = df['values'].apply(lambda x: np.max(x) if len(x) > 0 else 0)\n",
    "    df['values_median'] = df['values'].apply(lambda x: np.median(x) if len(x) > 0 else 0)\n",
    "    # Удаляем оригинальный столбец 'values'\n",
    "    df.drop('values', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_data = process_values(train_data)\n",
    "test_data = process_values(test_data)\n",
    "\n",
    "# Заполнение пропущенных значений\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "# Шаг 4: Подготовка данных для модели\n",
    "\n",
    "# Определяем числовые признаки\n",
    "numerical_cols = ['date_counts', 'date_range', 'values_mean', 'values_std', 'values_min', 'values_max', 'values_median']\n",
    "\n",
    "# Масштабирование признаков\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data[numerical_cols])\n",
    "\n",
    "train_data[numerical_cols] = scaler.transform(train_data[numerical_cols])\n",
    "test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
    "\n",
    "# Разделение данных на признаки и целевую переменную\n",
    "X = train_data[numerical_cols]\n",
    "y = train_data['label']\n",
    "\n",
    "# Шаг 5: Кросс-валидация и обучение модели\n",
    "\n",
    "# Кросс-валидация с XGBoost\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "xgb_scores = cross_val_score(xgb_model, X, y, cv=skf, scoring='roc_auc')\n",
    "print(\"\\nСредний ROC AUC для XGBoost (5-фолдовая кросс-валидация):\", np.mean(xgb_scores))\n",
    "\n",
    "# Кросс-валидация с логистической регрессией (базовая модель)\n",
    "logreg_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "logreg_scores = cross_val_score(logreg_model, X, y, cv=skf, scoring='roc_auc')\n",
    "print(\"Средний ROC AUC для логистической регрессии (5-фолдовая кросс-валидация):\", np.mean(logreg_scores))\n",
    "\n",
    "# Обучение XGBoost на всей обучающей выборке\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Шаг 6: Предсказание на тестовом наборе\n",
    "\n",
    "# Подготовка тестовых данных\n",
    "X_test = test_data[numerical_cols]\n",
    "\n",
    "# Предсказания вероятностей на тестовом наборе\n",
    "test_scores = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Гипотеза 1: Признак 'values_mean' сильно коррелирует с целевой переменной\n",
    "corr_values_mean = np.corrcoef(train_data['values_mean'], y)[0, 1]\n",
    "print(f\"Корреляция между 'values_mean' и целевой переменной: {corr_values_mean}\")\n",
    "\n",
    "# Гипотеза 2: Разброс значений 'values_std' влияет на вероятность принадлежности к классу 1\n",
    "corr_values_std = np.corrcoef(train_data['values_std'], y)[0, 1]\n",
    "print(f\"Корреляция между 'values_std' и целевой переменной: {corr_values_std}\")\n",
    "\n",
    "# Шаг 7: Формирование файла submission.csv\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'score': test_scores\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\nФайл submission.csv успешно создан.\")\n",
    "\n",
    "# Автоматическая загрузка файла submission.csv\n",
    "import shutil\n",
    "if os.path.exists('submission.csv'):\n",
    "    shutil.move('submission.csv', './submission.csv')\n",
    "    print(\"Файл submission.csv готов к скачиванию.\")\n",
    "\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166513e-824b-4386-969a-bf1b6489c667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
